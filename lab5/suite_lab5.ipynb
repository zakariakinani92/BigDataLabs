{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPqAxNq1C+HqFYlJTJb2bu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simosaoudi/BigDataLabs/blob/main/suite_lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdJG_ubZ4aLU"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "vMZ1Lcge5Gjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eYkVnEOvMSr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j"
      ],
      "metadata": {
        "id": "4G328Xsz5RIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "id": "GziLfNNQ5YeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"ColabSpark\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .getOrCreate()\n",
        "print(\" Spark est configuré avec succès !\")"
      ],
      "metadata": {
        "id": "6dMa0R2a5zAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark.version)"
      ],
      "metadata": {
        "id": "i6_xmCJs55Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(1, \"Alice\", 23), (2, \"Bob\", 30), (3, \"Charlie\", 29)]\n",
        "columns = [\"id\", \"nom\", \"age\"]\n",
        "df = spark.createDataFrame(data, columns)"
      ],
      "metadata": {
        "id": "EDxuwIdX59RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "MD9wMmyS6BOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()  # Structure du DataFrame\n",
        "df.select(\"nom\", \"age\").show()  # Sélection de colonnes\n",
        "df.filter(df.age > 25).show()  # Filtrage des données"
      ],
      "metadata": {
        "id": "Yt3MsAwe6GZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.csv(\"/content/transaction_data.csv\", header=True, inferSchema=True)\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "DQdKTIhu64Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "2wax4A256_Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df[\"Transaction Amount\"] > 1000).show()"
      ],
      "metadata": {
        "id": "xLsPqxB48FUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy(\"Transaction Type\").sum(\"Transaction Amount\").show()"
      ],
      "metadata": {
        "id": "WEu7ifjj8JdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.orderBy(df[\"Transaction Amount\"].desc()).show(5)"
      ],
      "metadata": {
        "id": "3JeWjhgh8M6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymongo"
      ],
      "metadata": {
        "id": "Ph3LyBry8P0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymongo pyspark"
      ],
      "metadata": {
        "id": "KlbDhMYHEQn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "MJe5EHRSLgoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "mongo_uri = \"mongodb+srv://saoudi:Simao%402434@saoudicluster.bprtv1q.mongodb.net/bankdb.transactions?retryWrites=true&w=majority\"\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MongoDBIntegration\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.3.0\") \\\n",
        "    .config(\"spark.mongodb.read.connection.uri\", mongo_uri) \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "daiYxi6xQKgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j81PlY9WTu5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import findspark\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "mongo_uri = \"mongodb+srv://saoudi:Simao%402434@saoudicluster.bprtv1q.mongodb.net/bankdb.transactions?retryWrites=true&w=majority\"\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MongoDBIntegration\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.3.0\") \\\n",
        "    .config(\"spark.mongodb.read.connection.uri\", mongo_uri) \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark est prêt avec le connecteur 10.3.0\")"
      ],
      "metadata": {
        "id": "qv2KcodbQ7Wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df_csv = spark.read.csv(\"/content/transaction_data.csv\", header=True, inferSchema=True)\n",
        "df_csv.show(5)"
      ],
      "metadata": {
        "id": "zkJAvZ0ZSg72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_csv.write.format(\"mongodb\") \\\n",
        "    .mode(\"append\") \\\n",
        "    .option(\"connection.uri\", mongo_uri) \\\n",
        "    .option(\"database\", \"bankdb\") \\\n",
        "    .option(\"collection\", \"transactions\") \\\n",
        "    .save()\n",
        "\n",
        "print(\"Succès ! Les données ont quitté Colab pour rejoindre MongoDB Atlas.\")"
      ],
      "metadata": {
        "id": "HfM1J7HmTNEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_atlas = spark.read.format(\"mongodb\").load()\n",
        "df_atlas.show(5)"
      ],
      "metadata": {
        "id": "yE9ZJI4wTzlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_atlas.groupBy(\"Device Used\").avg(\"Latency (ms)\").show()"
      ],
      "metadata": {
        "id": "kWJbuEAcU_oQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_atlas.printSchema()"
      ],
      "metadata": {
        "id": "jkLXKswAVdCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_atlas.groupBy(\"Transaction Type\").avg(\"Transaction Amount\").show()"
      ],
      "metadata": {
        "id": "T2K_zuaLVmhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_atlas.groupBy(\"Sender Account ID\").count().filter(\"count > 5\").show()"
      ],
      "metadata": {
        "id": "bBx_PTkkVr18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_atlas.createOrReplaceTempView(\"transactions\")\n",
        "print(\" Table 'transactions' prête pour les requêtes SQL\")"
      ],
      "metadata": {
        "id": "l_Fv98gAWH5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query1 = \"\"\"\n",
        "SELECT `Transaction Type`, SUM(`Transaction Amount`) as Total_Amount\n",
        "FROM transactions\n",
        "GROUP BY `Transaction Type`\n",
        "\"\"\"\n",
        "result1 = spark.sql(query1)\n",
        "result1.show()"
      ],
      "metadata": {
        "id": "VlPtNPmXWQMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Agréger les données avec Spark et convertir en Pandas\n",
        "# On utilise df_atlas qui contient vos données MongoDB\n",
        "df_grouped = df_atlas.groupBy(\"Transaction Type\").sum(\"Transaction Amount\").toPandas()\n",
        "\n",
        "# 2. Renommer les colonnes pour plus de clarté\n",
        "df_grouped.columns = [\"Transaction Type\", \"Total Amount\"]\n",
        "df_grouped.sort_values(by=\"Total Amount\", ascending=False, inplace=True)\n",
        "\n",
        "# 3. Visualisation avec Seaborn\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(data=df_grouped, x=\"Transaction Type\", y=\"Total Amount\", palette=\"coolwarm\")\n",
        "\n",
        "plt.title(\"Montant Total des Transactions par Type (Données MongoDB Atlas)\")\n",
        "plt.xlabel(\"Type de Transaction\")\n",
        "plt.ylabel(\"Montant Total (€)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iIRhasNQWqQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sélection et conversion\n",
        "df_pandas = df_atlas.select(\"Transaction Amount\").toPandas()\n",
        "\n",
        "# Visualisation\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(df_pandas[\"Transaction Amount\"], bins=30, kde=True, color=\"blue\")\n",
        "plt.title(\"Distribution des Montants des Transactions\")\n",
        "plt.xlabel(\"Montant (€)\")\n",
        "plt.ylabel(\"Nombre de Transactions\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0UuOEs1oXZo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Préparation des données agrégées\n",
        "df_status = df_atlas.groupBy(\"Transaction Status\").count().toPandas()\n",
        "df_status.columns = [\"Transaction Status\", \"Count\"]\n",
        "\n",
        "# Visualisation\n",
        "plt.figure(figsize=(7, 5))\n",
        "sns.barplot(data=df_status, x=\"Transaction Status\", y=\"Count\", palette=\"pastel\")\n",
        "plt.title(\"Nombre de Transactions Réussies vs Échouées\")\n",
        "plt.xlabel(\"Statut de la Transaction\")\n",
        "plt.ylabel(\"Nombre de Transactions\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yBMvRwZ1Xf7Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}