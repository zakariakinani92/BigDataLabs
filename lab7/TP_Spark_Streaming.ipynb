{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM47kpAm8N7OxkvmZTc3DBe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simosaoudi/BigDataLabs/blob/main/TP_Spark_Streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCEBWvPzzayL",
        "outputId": "79d0ee82-118f-45a3-f7b8-7dfd88474de9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Création du fichier source pour simulé par un fichier alimenté aléatoirement\n",
        "with open(\"data_stream.txt\", \"w\") as f:\n",
        "    f.write(\"\")"
      ],
      "metadata": {
        "id": "Th49P8xBzydW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Alimentation du fichier en continu (flux simulé)\n",
        "import time\n",
        "import threading\n",
        "import random\n",
        "\n",
        "messages = [\n",
        "    \"spark streaming dstream\",\n",
        "    \"spark spark streaming\",\n",
        "    \"big data spark\"\n",
        "]\n",
        "\n",
        "def generate_stream():\n",
        "    while True:\n",
        "        with open(\"data_stream.txt\", \"a\") as f:\n",
        "            f.write(random.choice(messages) + \"\\n\")\n",
        "        time.sleep(2)\n",
        "\n",
        "threading.Thread(target=generate_stream, daemon=True).start()\n",
        "# Une ligne est ajoutée toutes les 2 secondes pour simuler un flux temps réel.\n"
      ],
      "metadata": {
        "id": "Do3Uw5_Q0A4_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Création du SparkContext et du StreamingContext\n",
        "\n",
        "# SparkContext : point d’entrée Spark\n",
        "from pyspark import SparkContext\n",
        "\n",
        "#StreamingContext : gère le streaming\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "ssc = StreamingContext(sc, 5)\n",
        "# 5 secondes correspond à la durée du micro-batch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAmvL-vr0WSW",
        "outputId": "38b60b17-cb94-4b80-c1da-c135b63d080c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lecture des données sous forme de DStream\n",
        "\n",
        "# Spark surveille le dossier /content et lit les nouvelles données comme un DStream.\n",
        "lines = ssc.textFileStream(\"/content\")"
      ],
      "metadata": {
        "id": "VAHVmJfM0o2e"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Application du WordCount en streaming\n",
        "\n",
        "# Découpage des lignes en mots\n",
        "words = lines.flatMap(lambda line: line.split(\" \"))\n",
        "\n",
        "# Transformation en paires (mot, 1)\n",
        "pairs = words.map(lambda word: (word, 1))\n",
        "\n",
        "# Calcul du nombre d’occurrences par mot\n",
        "counts = pairs.reduceByKey(lambda a, b: a + b)\n"
      ],
      "metadata": {
        "id": "5b96PZ0601fU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Affichage des résultats dans la console\n",
        "\n",
        "# Les résultats sont affichés toutes les 5 secondes.\n",
        "counts.pprint()"
      ],
      "metadata": {
        "id": "op5CKcGi1F95"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Démarrage du Streaming\n",
        "\n",
        "# Le streaming s’exécute pendant 30 secondes.\n",
        "ssc.start()\n",
        "ssc.awaitTerminationOrTimeout(30)\n",
        "ssc.stop(stopSparkContext=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOfllKGA1PDC",
        "outputId": "b45cbe7a-abc0-42e8-cf5b-e44cb8cfab34"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2026-01-16 17:23:25\n",
            "-------------------------------------------\n",
            "('streaming', 104)\n",
            "('dstream', 51)\n",
            "('big', 52)\n",
            "('spark', 209)\n",
            "('data', 52)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2026-01-16 17:23:30\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2026-01-16 17:23:35\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2026-01-16 17:23:40\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2026-01-16 17:23:45\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2026-01-16 17:23:50\n",
            "-------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Qu’est-ce qu’un micro-batch dans DStream ?\n",
        "# Un micro-batch est un petit lot de données collectées pendant un intervalle de temps fixe et traité comme un batch classique."
      ],
      "metadata": {
        "id": "uZwn68S01yAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sur quelle structure repose un DStream ?\n",
        "# Un DStream repose sur une séquence de RDD."
      ],
      "metadata": {
        "id": "IC31NdA52MDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quelle est la durée du batch utilisée ?\n",
        "# ssc = StreamingContext(sc, 5)\n",
        "# la durée du batch utilisée est 5 seconde"
      ],
      "metadata": {
        "id": "bG3keL7W2Som"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}